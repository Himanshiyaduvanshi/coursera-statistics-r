[
["index.html", "Statistics with R Syllabus", " Statistics with R Cecilia Lee 2019-07-29 Syllabus https://www.coursera.org/specializations/statistics Introduction to Probability and Data Inferential Statistics Linear Regression and Modeling Bayesian Statistics Statistics with R Capstone "],
["week-1.html", "Week 1 Central Limit Theorem Exercises Confidence Interval Exercises", " Week 1 Central Limit Theorem Sampling Variability and CLT Example P(Length of song lasts more than 5 minutes)? # iPod song = 3000 n &lt;- 3000 # mean song length = 3.45 mins mu &lt;- 3.45 # sd of song length = 1.63 mins s &lt;- 1.63 # x = length of song lasts more than 5 mins # P(x &gt; 5) list(p_x_greater_5 = (350+100+25+20+5) / n) ## $p_x_greater_5 ## [1] 0.1666667 P(Average length of song lasts more than 6 minutes)? # sample size is 100 songs m &lt;- 100 # standard error = sd / sqrt(m) se &lt;- s/sqrt(m) # 6 hours = 360 mins # P(x1 + x2 + ... x100 &gt; 360 mins) # = P(mean x &gt;= 3.6 mins) list(p_xbar_greater_3.6 = 1 - pnorm(3.6, mu, se)) ## $p_xbar_greater_3.6 ## [1] 0.1787223 Exercises OpenIntro Statistics, 4th edition 5.1, 5.3, 5.5 5.1 Identify the parameter, Part I. For each of the following situations, state whether the parameter of interest is a mean or a proportion. It may be helpful to examine whether individual responses are numerical or categorical. In a survey, one hundred college students are asked how many hours per week they spend on the Internet. In a survey, one hundred college students are asked: “What percentage of the time you spend on the Internet is part of your course work?” In a survey, one hundred college students are asked whether or not they cited information from Wikipedia in their papers. In a survey, one hundred college students are asked what percentage of their total weekly spending is on alcoholic beverages. In a sample of one hundred recent college graduates, it is found that 85 percent expect to get a job within one year of their graduation date. # (a) Mean. The response is numerical - number of hours. # (b) Mean. The response is numerical - a percentage. # (c) Proportion. The response is a binary categorical - yes or no. # (d) Mean. The response is numerical - a percentage. # (e) Proportion. The response is a binary categorical - get a job or not get a job. 5.3 Quality control. As part of a quality control process for computer chips, an engineer at a factory randomly samples 212 chips during a week of production to test the current rate of chips with severe defects. She finds that 27 of the chips are defective. What population is under consideration in the data set? What parameter is being estimated? What is the point estimate for the parameter? What is the name of the statistic can we use to measure the uncertainty of the point estimate? Compute the value from part (d) for this context. The historical rate of defects is 10%. Should the engineer be surprised by the observed rate of defects during the current week? Suppose the true population value was found to be 10%. If we use this proportion to recompute the value in part (e) using p = 0.1 instead of ˆ p, does the resulting value change much? # (a) The population is all the computer chips during a week of production. # (b) The parameter is the rate of defects. # (c) Point estimate. list(p = round(27/212, 3)) ## $p ## [1] 0.127 # (d) Standard error / margin of error. # (e) SE = sqrt(p * (1-p) / n) list(se = round(sqrt((0.127 * (1-0.127)) / 212), 3)) ## $se ## [1] 0.023 # (f) We compute the 95% confidence interval which is between 0.08 and 0.17. # The historical rate of defects lie within our confidence interval hence we&#39;re not supprised. me &lt;- 1.96 * 0.023 list(ci = c(0.127 - me, 0.127 + me)) ## $ci ## [1] 0.08192 0.17208 # (g) The value does not change much. list(se = round(sqrt(0.1 * (1-0.1) / 212), 3)) ## $se ## [1] 0.021 5.5 Repeated water samples. A nonprofit wants to understand the fraction of households that have elevated levels of lead in their drinking water. They expect at least 5% of homes will have elevated levels of lead, but not more than about 30%. They randomly sample 800 homes and work with the owners to retrieve water samples, and they compute the fraction of these homes with elevated lead levels. They repeat this 1,000 times and build a distribution of sample proportions. What is this distribution called? Would you expect the shape of this distribution to be symmetric, right skewed, or left skewed? Explain your reasoning. If the proportions are distributed around 8%, what is the variability of the distribution? What is the formal name of the value you computed in (c)? Suppose the researchers’ budget is reduced, and they are only able to collect 250 observations per sample, but they can still collect 1,000 samples. They build a new distribution of sample proportions. How will the variability of this new distribution compare to the variability of the distribution when each sample contained 800 observations? # (a) Sampling distribution. # (b) If the population proportion is in the 5-30% range, the success-failure condition would be satisfied # and the sampling distribution would be symmetric. # (c) The variability can be represented by the standard error. mu &lt;- 0.08 list(se = round(sqrt(0.08 * (1-0.08) / 800), 4)) ## $se ## [1] 0.0096 # (d) Standard error. # (e) The variability will increase as the sample size decreases. Confidence Interval Confidence Interval (for a Mean) \\[ ME = z^\\star \\frac{s}{\\sqrt{n}} \\] 95% CI = mu +/- 1.96 se qnorm((1-0.95)/2) ## [1] -1.959964 98% CI = mu +/- 2.32 se qnorm((1-0.98)/2) ## [1] -2.326348 99% CI = mu +/- 2.58 se qnorm((1-0.99)/2) ## [1] -2.575829 Accuracy vs Precision Commonly used CI are 90%, 95%, 98%, and 99%. A wider interval (higher CI) indicates a higher probability of capturing the true polulation, which increases the accuracy, but decreases the precision. The way to get both a higher precision and higher accuracy is to increase the sample size, as it shrinks the standard error and margin of error. Example The General Social Survey (GSS) is a sociological survey used to collect data on demographic characteristics and attitudes of residents of the United States. In 2010, the survey collected responses from 1,154 US residents. Based on the survey results, a 95% confidence interval for the average number of hours Americans have to relax or pursue activities that you enjoy after an average work day is 3.53 to 3.83 hours. list( sample_mean = 3.53 + (3.83-3.53)/2, standard_error = (3.83-3.53)/2/1.96, margin_of_error = (3.83-3.53)/2 ) ## $sample_mean ## [1] 3.68 ## ## $standard_error ## [1] 0.07653061 ## ## $margin_of_error ## [1] 0.15 Required Sample Size for Margin of Error (ME) All else held constant, as sample size increases, the margin of error decreases. \\[ n = ( \\frac{z^\\star s}{ME} )^2 \\] Example Suppose a group of researchers want to test the possible effect of an epilepsy medication taken by pregnant mothers on the cognitive development of their children. As evidence, they want to estimate the IQs of three-year-old children born to mothers who were on this medication during their pregnancy. Previous studies suggest that the standard deviation of IQ scores of three-year-old children is 18 points. How many such children should the researches sample in order to obtain a 90% confidence interval with a margin of error less than or equal to four points? me &lt;- 4 ci &lt;- 0.9 sd &lt;- 18 z &lt;- qnorm((1-ci)/2) n &lt;- ((1.64 * sd)/me)^2 list( z = z, n = ceiling(n) ) ## $z ## [1] -1.644854 ## ## $n ## [1] 55 How would the required sample size change if we want to further decrease the margin of error, to two points? \\[ \\frac{1}{x} ME = z^\\star \\frac{s}{\\sqrt{n}} \\frac{1}{x} \\\\ \\frac{1}{x} ME = z^\\star \\frac{s}{\\sqrt{n x^2}} \\] me &lt;- 2 n &lt;- ((1.64 * sd)/me)^2 list(n = ceiling(n)) ## $n ## [1] 218 Example A sample of 50 college students were asked, how many exclusive relationships they’ve been in so far? The students in the sample had an average of 3.2 exclusive relationships, with a standard deviation of 1.74. In addition, the same distribution was only slightly skewed to the right. Estimate the true number of exclusive relationships based on this sample using a 95% confidence interval. n &lt;- 50 mu &lt;- 3.2 sd &lt;- 1.74 ci &lt;- 0.95 z &lt;- abs(round(qnorm((1-ci)/2), 2)) se &lt;- sd/sqrt(n) me &lt;- z * se # 1.96 * 1.74/sqrt(50) list(ci = c(mu - me, mu + me)) ## $ci ## [1] 2.717697 3.682303 What is the correct calculation of the 98% confidence interval for the average number of exclusive relationships college students on average have been in? ci &lt;- 0.98 z &lt;- abs(round(qnorm((1-ci)/2), 2)) me &lt;- z * se # 2.33 * 1.74/sqrt(50) list(ci = c(mu - me, mu + me)) ## $ci ## [1] 2.62665 3.77335 Exercises OpenIntro Statistics, 4th edition 5.7, 5.9, 5.11, 5.13 5.7 Chronic illness, Part I. In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”. However, this value was based on a sample, so it may not be a perfect estimate for the population parameter of interest on its own. The study reported a standard error of about 1.2%, and a normal model may reasonably be used in this setting. Create a 95% confidence interval for the proportion of U.S. adults who live with one or more chronic conditions. Also interpret the confidence interval in the context of the study. mu &lt;- 0.45 se &lt;- 0.012 # We are 95% confident that the proportion of U.S. adults who live with # one or more chronic conditions is between 42.6% to 47.4%. list(ci = c(mu - se * 1.96, mu + se * 1.96)) ## $ci ## [1] 0.42648 0.47352 5.9 Chronic illness, Part II. In 2013, the Pew Research Foundation reported that “45% of U.S. adults report that they live with one or more chronic conditions”, and the standard error for this estimate is 1.2%. Identify each of the following statements as true or false. Provide an explanation to justify each of your answers. We can say with certainty that the confidence interval from Exercise 5.7 contains the true percentage of U.S. adults who suffer from a chronic illness. If we repeated this study 1,000 times and constructed a 95% confidence interval for each study, then approximately 950 of those confidence intervals would contain the true fraction of U.S. adults who suffer from chronic illnesses. The poll provides statistically significant evidence (at the α = 0.05 level) that the percentage of U.S. adults who suffer from chronic illnesses is below 50%. Since the standard error is 1.2%, only 1.2% of people in the study communicated uncertainty about their answer. # (a) False. We are only 95% confident that the confidence interval from Exercise 5.7 # contains the true parameter. Within a range of plausible values, # sometimes the truth is missed. 5% of our samples misses the truth # under the 95% confidence interval. # (b) True. 950 out of 1000 samples represents 95% of of the samples. # (d) False. The standard error represents the variability between samples, # it describes the uncertainty in the overall point estimate due to randomness, # but not the uncertainty corresponding to individual&#39;s responses. 5.11 Waiting at an ER, Part I. A hospital administrator hoping to improve wait times decides to estimate the average emergency room waiting time at her hospital. She collects a simple random sample of 64 patients and determines the time (in minutes) between when they checked in to the ER until they were first seen by a doctor. A 95% confidence interval based on this sample is (128 minutes, 147 minutes), which is based on the normal model for the mean. Determine whether the following statements are true or false, and explain your reasoning. We are 95% confident that the average waiting time of these 64 emergency room patients is between 128 and 147 minutes. We are 95% confident that the average waiting time of all patients at this hospital’s emergency room is between 128 and 147 minutes. 95% of random samples have a sample mean between 128 and 147 minutes. A 99% confidence interval would be narrower than the 95% confidence interval since we need to be more sure of our estimate. The margin of error is 9.5 and the sample mean is 137.5. In order to decrease the margin of error of a 95% confidence interval to half of what it is now, we would need to double the sample size. # (a) False. We are 100% confident that the average waiting time of the sampled 64 patients # is 137.5. We are 95% confident about the polulation waiting time, but not the sample. # (b) True. If the samples are independent, and the success-failture condition is satisfied. # (c) False. The confidence interval is not about a sample mean. # (d) False. A 99% confidence interval would be wider since we have to capture more plausible # values of the true parameter. # (e) False. The mean is 137.5 which is the mid-point of the interval. # The margin of error is half the width of the interval. # (f) False. We have to have 4 times the sample size. 5.13 Website registration. A website is trying to increase registration for first-time visitors, exposing 1% of these visitors to a new site design. Of 752 randomly sampled visitors over a month who saw the new design, 64 registered. Check any conditions required for constructing a confidence interval. Compute the standard error. Construct and interpret a 90% confidence interval for the fraction of first-time visitors of the site who would register under the new design (assuming stable behaviors by new visitors over time). # (a) # As the visitors are randomly sampled, independence is assumed. # The success-failure condition is also satisfied. Hence, central limite theorem should # hold and the sampling distribution should follow a nearly normal distribution. p &lt;- 64/752 list( p = p, n_success = 752 * p, n_failure = 752 * (1-p) ) ## $p ## [1] 0.08510638 ## ## $n_success ## [1] 64 ## ## $n_failure ## [1] 688 # (b) list(se = round(sqrt(p * (1-p) / 752), 3)) ## $se ## [1] 0.01 # (c) z &lt;- abs(round(qnorm((1-0.9)/2), 3)) me &lt;- z * 0.01 list( z = z, ci = c(round(p - me, 3), round(p + me, 3)) ) ## $z ## [1] 1.645 ## ## $ci ## [1] 0.069 0.102 "],
["week-2.html", "Week 2 Hypothesis Testing Exercises Significance Exercises", " Week 2 Hypothesis Testing Hypothesis Testing Framework We start with a null hypothesis (H0) that represents the status quo. We also have an alternative hypothesis (HA) that represents our research question, i.e. what we’re testing for. We conduct a hypothesis test under the assumption that the null hypothesis is true, either via simulation or theorectical methods - methods that rely on the CLT. If the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in favor of the alternative. Hypothesis Testing (for a Mean) Hypothesis null - H0 : Ofen either a skeptical perspective or a claim to be tested (=) alternative - HA : Represents an alternative claim under consideration and is often represented by a range of possible parameter values. (&lt;, &gt;, !=) The skeptic will not abandon the H0 unless the evidence in favor of the HA is so strong that she rejects H0 in favor of HA. Hypothesis is always about population parameters, but never about sample statistics. P-Value P(observed or more extreme outcome | H0 true) We use the test statistic to calculate the p-value, the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis was true. If the p-value is low (lower than the significance level \\(\\alpha\\), which is usually 5%) we say that it would be very unlikely to observe the data if the null hypothesis were true, and hence reject H0. If the p-value is high (higher than \\(\\alpha\\)) we say that it is likely to observe the data even if the null hypothesis were true, and hence do not reject H0. Example Earlier we calculated a 95% confidence interval for the average number of exclusive relationships college students have been in to be 2.7 to 3.7. Based on this confidence interval, do these data support the hypothesis that college students on average have been in more than three exclusive relationships? \\[ P(\\bar{x} &gt; 3.2 | H_0 : \\mu = 3) \\] # H0: mu = 3 # Collage students have been in 3 exclusive relationships, on average. # HA: mu &gt; 3 # Collage students have been in more than 3 exclusive relationships, on average. # P(x_bar &gt; 3.2 | H0: mu = 3) # Since we assumme H0 is true, we can construct the sampling distribution based on the CLT. # x_bar ~ N(mu = 3, se = 0.246) # test statistic (z-score for normal distribution) z &lt;- round((3.2 - 3)/0.246, 2) # p-value p_value &lt;- round(1 - pnorm(z), 2) # Since p-value is high, we do not reject H0. list(z = z, p_value = p_value) ## $z ## [1] 0.81 ## ## $p_value ## [1] 0.21 # If in fact college students have been in 3 exclusive relationships on average (H0 true), there is a # 21% (0.21) chance that a random sample of 50 college student would yield a sample mean of 3.2 or higher. # This is a high probability, so we think that a sample mean of 3.2 or more exclusive relationships is # likely to happen simply by chance or sampling variability. Two-Sided Tests Often instead of looking for a divergence from the null in a specific direction, we might be interested in divergence in any direction. We call such hypothesis tests two-sided (or two-tailed). The definition of a p-value is the same regardless of doing a one or a two-sided test. However, the calculation becomes slightly different and ever so slightly more complicated since we need to consider “at least as extreme as the observed outcome” in both directions away from the mean. Example \\[ P(\\bar{x} &gt; 3.2 \\text{ OR } \\bar{x} &lt; 2.8 | H_0 : \\mu = 3) \\] # test statistic z_upper &lt;- round((3.2 - 3)/0.246, 2) z_lower &lt;- round((2.8 - 3)/0.246, 2) # p-value p_upper &lt;- round(1 - pnorm(z_upper), 3) p_lower &lt;- round(pnorm(z_lower), 3) p_value &lt;- p_upper + p_lower list( z = c(z_lower, z_upper), p_value = p_value ) ## $z ## [1] -0.81 0.81 ## ## $p_value ## [1] 0.418 Step-by-Step Set the hypotheses \\(H_0 : \\mu = \\text{null value}\\) \\(H_A: \\mu &lt; or &gt; or \\neq \\text{null value}\\) Calculate the point estimate: \\(\\bar{x}\\) Check conditions Independence Sample size/skew Draw sampling distribution, shape p-value, calculate test statistic $ z = $ $ SE = $ Make a decision, and interpret it in context of the research question If p-value &lt; \\(\\alpha\\), reject \\(H_0\\); the data provide convincing evidence for \\(H_A\\). If p-value &gt; \\(\\alpha\\), fail to reject \\(H_0\\) the data do not provide convincing evidence for \\(H_A\\). Example Researchers investigating characteristics of gifted children collected data from schools in a large city on a random sample of 36 children who were identified as gifted children soon after they reached the age of four. In this study, along with variables on the children, the researchers also collected data on their mothers’ IQ scores. The histogram shows the distribution of these data, and also provided our some sample statistics. n = 36 min = 101 mean = 118.2 sd = 6.5 max = 131 Perform a hypothesis test to evaluate if these data provide convincing evidence of a difference between the average IQ score of mothers of gifted children And the average IQ score for the population at large, which happens to be 100. We’re also asked to use a significance level of .01. # (1) Set the hypotheses # H0: mu = 100 # H1: mu != 100 # (2) Calculate the point estimate x_bar &lt;- 118.2 # (3) Check the conditions # random &amp; 35 &lt; 10% of all gifted child -&gt; independence # n &gt; 30 &amp; sample not skewed -&gt; nearly normal sampling distribution # (4) Sampling distribution, p-value, test statistic se &lt;- round(6.5/sqrt(36), 3) z_upper &lt;- (118.2-100)/se z_lower &lt;- (81.8-100)/se p_value &lt;- (pnorm(z_upper, lower.tail = FALSE)) + pnorm(z_lower) list( se = se, z = c(z_lower, z_upper), p_value = p_value ) ## $se ## [1] 1.083 ## ## $z ## [1] -16.80517 16.80517 ## ## $p_value ## [1] 2.236673e-63 # (5) Make a decision # p-value is very low -&gt; strong evidence against the null # We reject the null hypothesis and conclude that the data provide convincing evidence of a difference between # the average IQ score of mothers of gifted child and the average IQ score for the populatin at large. Example A statistics student interested in sleep habits of domestic cats took a random sample of 144 cats and monitored their sleep. The cats slept an average of 16 hours per day. According to our online resources, domestic dogs actually sleep on average 14 hours a day. We want to find out if these data provide convincing evidence of different sleeping habits for domestic cats and dogs with respect to how much they sleep. Note that the test statistic calculated was 1.73. What is the interpretation of this p-value in context of these data? # H0: mu = 14 # H1: mu != 14 list(p_value = pnorm(-1.73) * 2) ## $p_value ## [1] 0.08363028 # p(obtaining a random sample of 144 cats that sleep 16 hours or more or 12 hours or less, # on average, if in fact cats truly slept 14 hours per day on average) = 0.0836 # The p-value is larger than the significance level 0.05. Hence the evidence is not strong # enough to reject the null hypotheses. Hence, we cannot decide that there is a # difference between the sleeping habits for domestic cats and dogs. # If the average hours of sleep for domestic cat is 14, there is a 8% chance # that a random sample of size 144 would yield a sample mean of 16. Exercises OpenIntro Statistics, 3rd edition 4.17, 4.19, 4.23, 4.25, 4.27 4.17 Identify hypotheses, Part I. Write the null and alternative hypotheses in words and then symbols for each of the following situations. New York is known as “the city that never sleeps”. A random sample of 25 New Yorkers were asked how much sleep they get per night. Do these data provide convincing evidence that New Yorkers on average sleep less than 8 hours a night? Employers at a firm are worried about the effect of March Madness, a basketball championship held each spring in the US, on employee productivity. They estimate that on a regular business day employees spend on average 15 minutes of company time checking personal email, making personal phone calls, etc. They also collect data on how much company time employees spend on such non- business activities during March Madness. They want to determine if these data provide convincing evidence that employee productivity decreases during March Madness # (a) # H0: mu = 8 # HA: mu &lt; 8 # The null hypotheses assumes that the average sleep hours of New Yorkers # is no difference to the average sleep hours of other city # which is 8 hours. # The alternative hypotheses is that New Yorkers on average sleep less # than 8 hours. 4.19 Online communication. A study suggests that the average college student spends 10 hours per week communicating with others online. You believe that this is an underestimate and decide to collect your own sample for a hypothesis test. You randomly sample 60 students from your dorm and find that on average they spent 13.5 hours a week communicating with others online. A friend of yours, who offers to help you with the hypothesis test, comes up with the following set of hypotheses. Indicate any errors you see. \\[H_0 : \\bar{x} &lt; 10 \\text{ hours}\\] \\[H_A : \\bar{x} &gt; 13.5 \\text{ hours}\\] # The null hypotheses should be mu = 10 hours. # The alternative hypotheses should be mu &gt; 10 hours. 4.23 Nutrition labels. The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random sample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. Is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips? We have verified the independence, sample size, and skew conditions are satisfied. # H0: mu = 130 # HA: mu != 130 x &lt;- 130 p &lt;- 134 sd &lt;- 17 n &lt;- 35 se &lt;- sd/sqrt(n) z &lt;- (134-130)/se p_value &lt;- pnorm(z, lower.tail = FALSE) * 2 # If the null hypotheses is true (mean is 130), there is a 0.164 # chance during a random sample of 130 yielding a sample mean of 134. # Let alpha be 0.05, the p-value is higher than the significance # level and we failed to reject the null hypotheses. Hence, # there is no strong evidence suggesting the measure of calories in the # bags of potato chips is inaccurate. list(se = se, z = z, p_value = p_value) ## $se ## [1] 2.873524 ## ## $z ## [1] 1.392019 ## ## $p_value ## [1] 0.1639167 4.25 Waiting at an ER, Part III. The hospital administrator mentioned in Exercise 4.13 randomly selected 64 patients and measured the time (in minutes) between when they checked in to the ER and the time they were first seen by a doctor. The average time is 137.5 minutes and the standard deviation is 39 minutes. She is getting grief from her supervisor on the basis that the wait times in the ER has increased greatly from last year’s average of 127 minutes. However, she claims that the increase is probably just due to chance. Are conditions for inference met? Note any assumptions you must make to proceed. Using a significance level of α = 0.05, is the change in wait times statistically significant? Use a two-sided test since it seems the supervisor had to inspect the data before she suggested an increase occurred. Would the conclusion of the hypothesis test change if the significance level was changed to α = 0.01? # (a) # Independence: Since the patients are randomly selected, the independence # condition is met, and should make up of less than 10% of the ER residents. # Sample size: The sample size is 64, greater than 30. It&#39;s large enough # to perform inference with normal distribution. # Skewness: There&#39;s no information regarding the skewness but # we could assume that it&#39;s not skewed. # (b) # H0: mu = 127 # HA: mu != 127 n &lt;- 64 x &lt;- 137.5 p &lt;- 127 sd &lt;- 39 se &lt;- sd / sqrt(n) z &lt;- (p - x)/se p_value &lt;- pnorm(z)*2 # The p-value 0.03 is smaller than the significance level of 0.05. # Hence, the null hypotheses can be rejected in favor of the # alternative hypotheses. The data provide convincing evidence that # the average ER wait time has increased over the last year. list(se = se, z = z, p_value = p_value) ## $se ## [1] 4.875 ## ## $z ## [1] -2.153846 ## ## $p_value ## [1] 0.03125224 # (c) # If the alpha is changed to 0.01, the p-value is larger than the # significance level and hence we cannot reject the null hypotheses. 4.27 Working backwards, one-sided. You are given the following hypotheses: \\[H_0 : µ = 30\\] \\[H_A : µ &gt; 30\\] We know that the sample standard deviation is 10 and the sample size is 70. For what sample mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied. n &lt;- 70 x &lt;- 30 sd &lt;- 10 se &lt;- 10/sqrt(70) # Find the required z value if p-value = 0.05 z &lt;- qnorm(0.05) # Find the required mean # z = (p - x)/se required_mean &lt;- z * se + x list(se = se, z = z, required_mean = required_mean) ## $se ## [1] 1.195229 ## ## $z ## [1] -1.644854 ## ## $required_mean ## [1] 28.03402 Significance Inference for Other Estimators Any nearly normal sampling distributions Sample mean \\(\\bar{x}\\) Difference between sample means \\(\\bar{x}_1 - \\bar{x}_2\\) Sample proportion \\(\\hat{p}\\) Diffefference between sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) Unbiased estimator An important assumption about the point estimates is that they’re unbiased, i.e. the sampling distribution of the estimate is centered at the true population parameter it estimates. An unbiased estimate does not naturally over or underestimate the parameter but instead it provides a good estimate. We know that the sample mean is an example of an unbiased point estimate. Confidence Intervals Confidence intervals for nearly normal point estimates \\[ \\text{point estimate} \\pm z^{\\star} \\times SE \\] Hypothesis Testing Hypothesis testing for nearly normal point estimates \\[ Z = \\frac{\\text{ point estimate } - \\text{ null value }}{SE} \\] Example A 2010 Pew Research foundation poll indicates that among 1,099 college graduates, 33% watch the Daily Show. An American late-night TV Show. The standard error of this estimate is 0.014. We are asked to estimate the 95% confidence interval for the proportion of college graduates who watch The Daily Show. p &lt;- 0.33 se &lt;- 0.014 list(ci = c(p + 1.96 * se, p - 1.96 * se)) ## $ci ## [1] 0.35744 0.30256 Example The 3rd national health and nutrition examination survey NHANES, collected body fat percentage and gender data from over 13,000 subjects in ages between 20 to 80. The average body fat percentage for the 6,580 men in the sample was 23.9%. And this value was 35% for the, for the 7,021 women. The standard error for the difference between the average male and female body fat percentages was 0.114. Do these data provide convincing evidence that men and women have different average body fat percentages? You may assume that the distribution of the point estimate is nearly normal. # H0: mu_men = mu_women -&gt; mu_men - mu_women = 0 # H1: mu_men = mu_women -&gt; mu_men - mu_women != 0 # Point estimate # mu_men - mu_women p &lt;- 23.9 - 35 mu &lt;- 0 se &lt;- 0.114 z &lt;- (-11.1 - 0)/0.114 p_value &lt;- pnorm(z) * 2 # Reject the null hypothesis. list(p = p, z = z, p_value = p_value) ## $p ## [1] -11.1 ## ## $z ## [1] -97.36842 ## ## $p_value ## [1] 0 Decision Errors Type I error (\\(\\alpha\\)) is rejecting the H0 when H0 is true. (Declaring the defendant guilty when they are actually innocent.) Type II error (\\(\\beta\\)) is failing to reject H0 when HA is true. (Declaring the defendant innocent when they are actually guilty.) Power (\\(1 - \\beta\\)) of a test is the probability of correctly rejecting H0. We (almost) never know if H0 or HA is true, but we need to consider all possibilities. Type I Error Rate We reject the null hypothesis when the p-value is less than 0.05 (\\(\\alpha = 0.05\\)). This means that, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject it more than 5% of those times. In other words, when using a 5% significance level, there is about a 5% chance of making a type one error if the null hypothesis is true. \\[ P(\\text{Type I error } | H_0 \\text{ true}) = \\alpha \\] This is why we prefer small values of \\(\\alpha\\) - increasing \\(\\alpha\\) increases the Type I error rate. If Type I Error is dangerous or especially costly, choose a small significance level (e.g. 0.01). Goal: We want to be very cautious about rejecting H0, so we demand very strong evidence favoring HA before we should do so. If Type II Error is relatively more dangerous or much more costly, choose a higher significance level (e.g. 0.10). Goal: We want to be cautious about failing to reject H0 when the null is actually false. Type II Error Rate If the alternative hypothesis is actually true, what is the chance that we make a type two error? In other words, what is the chance that we fail to reject the null hypothesis, even when we should reject it? The answer to this is not obvious. If the true population average is very close to the null value, it will be very difficult to detect a difference and to reject the null hypothesis. In other words, if the true population average is very different from the null value, it will be much easier to detect a difference. Clearly then, \\(\\beta\\), the probability of making a type two area depends on our effect size. An effect size is defined as the difference between the point estimate and the null value. Significance vs Confidence Level Broadly we can say that a significance level and a confidence level are complements of each other. A two sided hypothesis with threshold of \\(\\alpha\\) is equivalent to a confidence interval with \\(CL = 1 - \\alpha\\). A one sided hypothesis with threshold of \\(\\alpha\\) is equivalent to a confidence interval with \\(CL = 1 - (2 \\times \\alpha)\\). If H0 is rejected, a confidence interval that agrees with the result of the hypothesis test should not include the null value. If H0 is failed to be rejected, a confidence interval that agrees with the result of the hypothesis test should include the null value. Statistical vs Practical Significance Real differences between the point estimate and the null value are easier to detect with large samples. However very large samples will result in statistical significance even for tiny differences between the sample mean and the null value or our effect size, even when the difference is not practically significant. Exercises OpenIntro Statistics, 3rd edition 4.43, 4.45 4.29, 4.31, 4.47 4.43 Spam mail counts. The 2004 National Technology Readiness Survey sponsored by the Smith School of Business at the University of Maryland surveyed 418 randomly sampled Americans, asking them how many spam emails they receive per day. The survey was repeated on a new random sample of 499 Americans in 2009. What are the hypotheses for evaluating if the average spam emails per day has changed from 2004 to 2009. In 2004 the mean was 18.5 spam emails per day, and in 2009 this value was 14.9 emails per day. What is the point estimate for the difference between the two population means? A report on the survey states that the observed difference between the sample means is not statistically significant. Explain what this means in context of the hypothesis test and data. Would you expect a confidence interval for the difference between the two population means to contain 0? Explain your reasoning. # (a) # H0: mu_2009 - mu_2004 = 0 # HA: mu_2009 - mu_2004 != 0 # (b) list(p = 18.5 - 14.9) ## $p ## [1] 3.6 # (c) # It means that assuming the null hypotheses is true, where the difference # between the average spam emails per day in 2004 and 2009 is 0, # the probability of observing a sample difference of 3.6 # is higher than the significance level alpha, in other words, # not rare. Hence, we cannot reject the null hypotheses and say # that the data provides statistically strong evidence in favor of # the alternative hypotheses. # (d) # Yes. Since the result is not statistically significant, we cannot # reject the null hypotheses. Hence, we would expect 0 to be # include in our confidence interval, i.e. it&#39;s highly plausible # to see the value 0. 4.45 Spam mail percentages. The National Technology Readiness Survey sponsored by the Smith School of Business at the University of Maryland surveyed 418 randomly sampled Americans, asking them how often they delete spam emails. In 2004, 23% of the respondents said they delete their spam mail once a month or less, and in 2009 this value was 16%. What are the hypotheses for evaluating if the proportion of those who delete their email once a month or less has changed from 2004 to 2009? What is the point estimate for the difference between the two population proportions? A report on the survey states that the observed decrease from 2004 to 2009 is statistically significant. Explain what this means in context of the hypothesis test and the data. Would you expect a confidence interval for the difference between the two population proportions to contain 0? Explain your reasoning. # (a) # H0: p_2004 = p_2009 # HA: p_2004 != p_2009 # (b) list(p = 0.16 - 0.23) ## $p ## [1] -0.07 # (c) # It means that assuming the null hypotheses is true, the probability # of observing a difference of 0.07 is very small, and smaller than # the significance level, hence we reject the null hypotheses # and say that the data provides evidence that there are difference # between the two population proportions, and the difference is not due # to sampling variability. # (d) # No. As we rejected the null hypotheses. 4.29 Testing for Fibromyalgia. A patient named Diana was diagnosed with Fibromyalgia, a long-term syndrome of body pain, and was prescribed anti-depressants. Being the skeptic that she is, Diana didn’t initially believe that anti-depressants would help her symptoms. However after a couple months of being on the medication she decides that the anti-depressants are working, because she feels like her symptoms are in fact getting better. Write the hypotheses in words for Diana’s skeptical position when she started taking the anti-depressants. What is a Type 1 Error in this context? What is a Type 2 Error in this context? # (a) # H0: The anti-depressants are not working. # HA: The anti-depressants are working. # (b) # Type 1 error is to declare the anti-depressants to be working # when the truth is they are useless. (c) ## function (...) .Primitive(&quot;c&quot;) # Type 2 error is to declare the anti-depressants to be useless # when the truth is they are helping the symptoms. 4.31 Which is higher? In each part below, there is a value of interest and two scenarios (I and II). For each part, report if the value of interest is larger under scenario I, scenario II, or whether the value is equal under the scenarios. The standard error of \\(\\bar{x}\\) when s = 120 and (I) n = 25 or (II) n = 125. The margin of error of a confidence interval when the confidence level is (I) 90% or (II) 80%. The p-value for a Z-statistic of 2.5 when (I) n = 500 or (II) n = 1000. The probability of making a Type 2 Error when the alternative hypothesis is true and the significance level is (I) 0.05 or (II) 0.10. # (a) # SE of mean is inversely relating to sample size. SE is larger in (I). # (b) # The margin of error depends on the z-score calculated by the confidence # interval and the standard error. # A higher confidence interval yield a higher z-score and hence a higher # margin of error. (I) has a higher margin of error. list(z_90 = qnorm((1-.9)/2), z_80 = qnorm((1-.8)/2)) ## $z_90 ## [1] -1.644854 ## ## $z_80 ## [1] -1.281552 # (d) # Type 2 error is failing to reject the null hypotheses when the # alternative hypotheses is true. In other words, # declare a person innocent when he&#39;s actually guilty. # When significance level is small, it&#39;s harder to declare a person guilty, # or to reject the null hypotheses. Hence, we&#39;re easier to # declare a person innocent when he&#39;s guilty. # (I) has a higher probability of making type 2 error. 4.47 Practical vs. statistical. Determine whether the following statement is true or false, and explain your reasoning: “With large sample sizes, even small differences between the null value and the point estimate can be statistically significant.” # True. n1 &lt;- 1000 n2 &lt;- 50 x &lt;- 50 p &lt;- 52 sd &lt;- 10 se1 &lt;- sd/sqrt(n1) z1 &lt;- (1-0)/se1 p_value_1 &lt;- pnorm(z1, lower.tail = FALSE) se2 &lt;- sd/sqrt(n2) z2 &lt;- (1-0)/se2 p_value_2 &lt;- pnorm(z2, lower.tail = FALSE) # For large sample size, the standard error will be lower, # and hence a larger test statistics and smaller p value. list( p_value_n1000 = p_value_1, p_value_n50 = p_value_2 ) ## $p_value_n1000 ## [1] 0.0007827011 ## ## $p_value_n50 ## [1] 0.2397501 "],
["week-3.html", "Week 3 T-Distribution Exercises ANOVA and Bootstrapping Exercises", " Week 3 T-Distribution T-Distribution When \\(\\sigma\\) is unknown (almost always), use the t-distribution to address the uncertainty of the standard error estimate Bell shaped but thicker tails than the normal distribution Observations more likely to fall beyond 2 SDs from the mean (more conservative) Extra thick tails helpful for mitigating the effect of a less reliable estimate for the standard error of the sampling distribution Always centered at 0 (like the standard normal) One parameter: degrees of freedom (df) - determines the thickness of tails (normal distribution has two parameters: mean and SD) When degrees of freedom increases, the shape of the t-distribution approaches the normal distribution T-Score \\[ T = \\frac{obs-null}{SE} \\] # P(|Z| &gt; 2) pnorm(2, lower.tail = FALSE) * 2 ## [1] 0.04550026 # P(|t_df=50| &gt; 2) pt(2, df = 50, lower.tail = FALSE) * 2 ## [1] 0.05094707 # P(|t_df=10| &gt; 2) pt(2, df = 10, lower.tail = FALSE) * 2 ## [1] 0.07338803 Inference for a Mean Confidence Interval \\[ \\bar{x} \\pm t^{\\star}_{df} SE_\\bar{x} \\\\ \\bar{x} \\pm t^{\\star}_{df} \\frac{s}{\\sqrt{n}} \\\\ \\bar{x} \\pm t^{\\star}_{n-1} \\frac{s}{\\sqrt{n}} \\] Degrees of Freedom for T-Statistic for Inference on One Sample Mean \\[ df = n - 1 \\] # Critical t-score for 0.95 confidence interval with df = 21 qt((1-0.95)/2, df = 21) ## [1] -2.079614 Example Suppose the suggested serving of these biscuits is 30 grams. Do these data provide convincing evidence that the amount of snacks consumed by distracted eaters post lunch is different than the suggested serving size? x̄ = 52.1 s = 45.1 n = 22 t_21 = 2.08 # Confidence interval list(ci = c(52.1 - 2.08 * 45.1/sqrt(22), 52.1 + 2.08 * 45.1/sqrt(22))) ## $ci ## [1] 32.10007 72.09993 # H0: mu = 30 # HA: mu != 30 # T-score t &lt;- (52.1 - 30) / (45.1/sqrt(22)) # P-value p_value &lt;- pt(t, df = 21, lower.tail = FALSE) * 2 list(t = t, p_value = p_value) ## $t ## [1] 2.298408 ## ## $p_value ## [1] 0.03190849 Inference for Comparing Two Independent Means Confidence Interval \\[ (\\bar{x}_1 - \\bar{x}_2) \\pm t^{\\star}_{df} SE_{(\\bar{x}_1 - \\bar{x}_2)} \\\\ (\\bar{x}_1 - \\bar{x}_2) \\pm t^{\\star}_{df} \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\\\ (\\bar{x}_1 - \\bar{x}_2) \\pm t^{\\star}_{min(n_1 - 1, n_2 - 1）} \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\] Standard Error of Difference between Two Independent Means \\[ SE_{(\\bar{x}_1 - \\bar{x}_2)} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}} \\] Degrees of Freedom for T-Statistic for Inference on Difference of Two Means \\[ df = min(n_1 - 1, n_2 - 1） \\] Example Solitaire x̄ = 52.1 s = 45.1 n = 22 No Distraction x̄ = 27.1 s = 26.4 n = 22 # Confidence interval df = 22-1 t_21 = qt((1-0.95)/2, df = 21) se = sqrt(45.1^2/22 + 26.4^2/22) ci = c((52.1 - 27.1) - 2.08 * se, (52.1 - 27.1) + 2.08 * se) # We are 95% confident that those who eat with distractions # consume 1.83 g and 48.17 g more snacks than those # who eat without distractions, on average. list( df = df, t_21 = t_21, se = se, ci = ci ) ## $df ## [1] 21 ## ## $t_21 ## [1] -2.079614 ## ## $se ## [1] 11.14159 ## ## $ci ## [1] 1.825495 48.174505 # H0: mu1 - mu2 = 0 # HA: mu1 - mu2 != 0 # T-score t &lt;- ((52.1 - 27.1) - 0) / se # P-value p_value &lt;- pt(t, df = 21, lower.tail = FALSE) * 2 list(t = t, p_value = p_value) ## $t ## [1] 2.243845 ## ## $p_value ## [1] 0.03575082 Inference for Comparing Two Paired Means Same as the inference for a single population mean, only the mean is a difference between the two paired means. \\(\\mu_{\\text{diff}}\\) : Parameter of Interest \\(\\bar{x}_{\\text{diff}}\\) : Point Estimate Example x̄_diff = -0.545 s_diff = 8.887 n_diff = 200 # H0: mu_diff = 0 # HA: mu_diff != 0 # Standard error se &lt;- 8.887 / sqrt(200) # T-score t &lt;- (-0.545 - 0) / se # P-value p_value &lt;- pt(t, df = 199) * 2 list(se = se, t = t, p_value = p_value) ## $se ## [1] 0.6284058 ## ## $t ## [1] -0.867274 ## ## $p_value ## [1] 0.3868365 Power \\(\\alpha\\) : Type I error - P(reject H0 | H0 true) \\(\\beta\\) : Type II error - P(fail to reject H0 | HA true) \\(1 - \\beta\\) : Power - P(reject H0 | HA true) Example sd = 12 n = 100 (per group) # H0: mu1 - mu2 = 0 # HA: mu1 - mu2 != 0 # Standard error list(se = sqrt(12^2/100 + 12^2/100)) ## $se ## [1] 1.697056 For what values of difference would we reject the null hypothesis at the 5% significance level? 1.96 * 1.7 ## [1] 3.332 Suppose the company cares about finding any effect that is 3mmHg or larger vs the standard medication. What is the power of the test that can detect this effect. effect size = -3 # Distribution with mu1 - mu2 = -3 z &lt;- (-3.332 - (-3)) / 1.7 # Power of the test power &lt;- pnorm(z) list(z = z, power = power) ## $z ## [1] -0.1952941 ## ## $power ## [1] 0.4225814 What sample size will lead to a power of 80% for this test? # Z-score that marks the 80th percentile of the normal curve qnorm(0.8) ## [1] 0.8416212 Exercises OpenIntro Statistics, 3rd edition 5.1, 5.3, 5.5, 5.13, 5.17, 5.19, 5.21, 5.23, 5.27, 5.31, 5.35, 5.37 5.39 5.41, 5.43, 5.45, 5.47, 5.49, 5.51 5.1 Identify the critical t. An independent random sample is selected from an approximately normal population with unknown standard deviation. Find the degrees of freedom and the critical t-value (t*) for the given sample size and confidence level. n = 6, CL = 90% n = 21, CL = 98% n = 29, CL = 95% n = 12, CL = 99% # (a) qt(.05, 5) ## [1] -2.015048 # (b) qt(.01, 20) ## [1] -2.527977 # (c) qt(.025, 28) ## [1] -2.048407 # (d) qt(.005, 11) ## [1] -3.105807 5.3 Find the p-value, Part I. An independent random sample is selected from an approximately normal population with an unknown standard deviation. Find the p-value for the given set of hypotheses and T test statistic. Also determine if the null hypothesis would be rejected at α = 0.05. HA : μ &gt; μ0, n = 11, T = 1.91 HA : μ &lt; μ0, n = 17, T = −3.45 HA : μ != μ0, n = 7, T = 0.83 HA : μ &gt; μ0, n = 28, T = 2.1 # (a) # H0 rejected pt(1.91, 10, lower.tail = FALSE) ## [1] 0.04260244 # (b) # H0 rejected pt(-3.45, 16) ## [1] 0.001646786 # (c) # H0 not rejected pt(0.83, 6, lower.tail = FALSE) * 2 ## [1] 0.4383084 # (d) # H0 rejected pt(2.1, 27, lower.tail = FALSE) ## [1] 0.02260436 5.5 Working backwards, Part I. A 95% confidence interval for a population mean, μ, is given as (18.985, 21.015). This confidence interval is based on a simple random sample of 36 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations # t t &lt;- qt(.025, 35) # me me &lt;- (21.015 - 18.985) / 2 # me = t * s / sqrt(n) # 1.015 = 2.03 * s / sqrt(36) # s s &lt;- 1.015 / 2.03 * sqrt(36) # mean x &lt;- 18.985 + 1.015 list(t = t, me = me, s = s, mean = x) ## $t ## [1] -2.030108 ## ## $me ## [1] 1.015 ## ## $s ## [1] 3 ## ## $mean ## [1] 20 5.13 Car insurance savings. A market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is 100. He wants to collect data such that he can get a margin of error of no more than 10 at a 95% confidence level. How large of a sample should he collect? # s = 100 # me = 10 # me = z * s / sqrt(n) # 10 = 1.96 * 100 / sqrt(n) (1.96 * 100 / 10)^2 ## [1] 384.16 5.17 Paired or not, Part I? In each of the following scenarios, determine if the data are paired. Compare pre- (beginning of semester) and post-test (end of semester) scores of students. Assess gender-related salary gap by comparing salaries of randomly sampled men and women. Compare artery thicknesses at the beginning of a study and after 2 years of taking Vitamin E for the same group of patients. Assess effectiveness of a diet regimen by comparing the before and after weights of subjects. # (a) # Yes, pre- and post-test scores are paired because they&#39;re scores of the same student, and thus are not independent. # (b) # No, as data are randomly sampled, they&#39;re assuemed to be independent. # (c) # Yes, because they&#39;re of the same group of patients. # (d) # Yes, because they&#39;re of the same subjects. 5.19 Global warming, Part I. Is there strong evidence of global warming? Let’s consider a small scale example, comparing how temperatures have changed in the US from 1968 to 2008. The daily high temperature reading on January 1 was collected in 1968 and 2008 for 51 randomly selected locations in the continental US. Then the difference between the two readings (temperature in 2008 - temperature in 1968) was calculated for each of the 51 different locations. The average of these 51 values was 1.1 degrees with a standard deviation of 4.9 degrees. We are interested in determining whether these data provide strong evidence of temperature warming in the continental US. Is there a relationship between the observations collected in 1968 and 2008? Or are the observations in the two groups independent? Explain. Write hypotheses for this research in symbols and in words. Check the conditions required to complete this test. Calculate the test statistic and find the p-value. What do you conclude? Interpret your conclusion in context. What type of error might we have made? Explain in context what the error means. Based on the results of this hypothesis test, would you expect a confidence interval for the average difference between the temperature measurements from 1968 and 2008 to include 0? Explain your reasoning. # (a) # The observations collected in 1968 and 2008 are not independent. # Although the 51 locations are randomly selected, the observations collected # in two different years are from the same locations. # The data are paired. # (b) # H0: mu_diff = 0 # HA: mu_diff &gt; 0 # The null hypothesis assume that the mean of the difference in temperature in 2008 and 1968 is 0. # The alternative hypothesis assume that the mean of the difference greater than 0, i.e. warming. # (d) n = 51 x = 1.1 s = 4.9 se &lt;- 4.9 / sqrt(n) t &lt;- 1.1 / se p_value &lt;- pt(t, 50, lower.tail = FALSE) list(se = se, t = t, p_value = p_value) ## $se ## [1] 0.6861372 ## ## $t ## [1] 1.603178 ## ## $p_value ## [1] 0.05759731 # (e) # Assume the significance level to be 0.05, the p-value is not large enough to reject the null hypothesis. # Hence, there is not enough evidence to say that the temperature is warming in the continental US. # (f) # Type II error. There is actually temperature warming but we failed to reject the null hypothesis. # (g) # Yes. Since the null hypothesis is not rejected, the null hypothesis assume that the mean is 0. # Hence the confidence interval should include 0. 5.21 Global warming, Part II. We considered the differences between the temperature readings in January 1 of 1968 and 2008 at 51 locations in the continental US in Exercise 5.19. The mean and standard deviation of the reported differences are 1.1 degrees and 4.9 degrees. Calculate a 90% confidence interval for the average difference between the temperature measurements between 1968 and 2008. Interpret this interval in context. Does the confidence interval provide convincing evidence that the temperature was higher in 2008 than in 1968 in the continental US? Explain # (a) # t t &lt;- qt(0.05, 50) # me me &lt;- 1.6759 * se list(t = t, me = me, ci = c(1.1 - me, 1.1 + me)) ## $t ## [1] -1.675905 ## ## $me ## [1] 1.149897 ## ## $ci ## [1] -0.0498974 2.2498974 # (b) # We are 90% confident that the average difference between the temperature measurements between 1968 and 2008 # with a sample size of 51 is between -0.05 and 2.25. # (c) # The confidence interval also has a negative range. Hence there is no convincing evidence that the temperature # was higher in 2008 than in 1968. 5.23 Gifted children. Researchers collected a simple random sample of 36 children who had been identified as gifted in a large city. The following histograms show the distributions of the IQ scores of mothers and fathers of these children. Also provided are some sample statistics. / Mother Father Diff Mean 118.2 114.8 3.4 SD 6.5 3.5 7.5 n 36 36 36 Are the IQs of mothers and the IQs of fathers in this data set related? Explain. Conduct a hypothesis test to evaluate if the scores are equal on average. Make sure to clearly state your hypotheses, check the relevant conditions, and state your conclusion in the context of the data # (a) # Yes. Since IQ could be a factor affecting marriage. The IQ of mothers and fathers are paired. # (b) # H0: mu_diff = 0 # HA: mu_diff != 0 # The null hypothesis assumes that there are no difference between the average IQ of mother and father. # The alternative assumes that there are difference between the average IQ of mother and father. # A random sample of 36 obviously will be less than 10% of the population of a large city. # The distribution of IQ difference is slightly skewed. # But the sample size of 36 is large enough to meet the condition. se &lt;- 7.5/sqrt(36) t &lt;- (3.4-0)/se p_value &lt;- pt(t, 35, lower.tail = FALSE) * 2 list(se = se, t = t, p_value = p_value) ## $se ## [1] 1.25 ## ## $t ## [1] 2.72 ## ## $p_value ## [1] 0.01009512 # With a significance level of 0.05, a p-value of 0.01 is small enough to reject the null hypothesis # and conclude that our data provide strong evidence that there are difference between the average IQ of mother and father, # and the data indicate that mothers’ scores are higher than fathers’ scores for the parents of gifted children. 5.27 Friday the 13th, Part I. In the early 1990’s, researchers in the UK collected data on traffic flow, number of shoppers, and traffic accident related emergency room admissions on Friday the 13th and the previous Friday, Friday the 6th. The histograms below show the distribution of number of cars passing by a specific intersection on Friday the 6th and Friday the 13th for many such date pairs. Also given are some sample statistics, where the difference is the number of cars on the 6th minus the number of cars on the 13th. / 6th 13th Diff Mean 128385 126550 1835 SD 7259 7664 1176 n 10 10 10 Are there any underlying structures in these data that should be considered in an analysis? Explain. What are the hypotheses for evaluating whether the number of people out on Friday the 6th is different than the number out on Friday the 13th? Check conditions to carry out the hypothesis test from part (b). Calculate the test statistic and the p-value. What is the conclusion of the hypothesis test? Interpret the p-value in this context. What type of error might have been made in the conclusion of your test? Explain. # (a) # The number of cars on the 6th and the number of cars on the 13th should be paired. # (b) # H0: mu_diff = 0 # HA: mu_diff != 0 # (d) df = 10 - 1 se = 1176/sqrt(10) t = (1835 - 0) / se p_value = pt(t, df, lower.tail = FALSE) * 2 list(df = df, se = se, t = t, p_value = p_value) ## $df ## [1] 9 ## ## $se ## [1] 371.8839 ## ## $t ## [1] 4.934336 ## ## $p_value ## [1] 0.0008085065 # (e) # The p-value of 0.0008 is much smaller than the significance level of 0.05 # and hence reject the null hypothesis. # (f) # The p-value is the probability of observing a difference of the mean of the number of cars # on 6th and 13th as large as the observation difference under # the assumption that the null hypothesis is true, i.e. there are no difference. # (g) # Type I error. There might actually be no difference but we wrongly # rejected the null hypothesis and state that there is a difference. 5.31 Chicken diet and weight, Part I. Chicken farming is a multi-billion dollar industry, and any methods that increase the growth rate of young chicks can reduce consumer costs while increasing company profits, possibly by millions of dollars. An experiment was conducted to measure and compare the effectiveness of various feed supplements on the growth rate of chickens. Newly hatched chicks were randomly allocated into six groups, and each group was given a different feed supplement. Below are some summary statistics from this data set along with box plots showing the distribution of weights by feed type. / Mean SD n casein 323.58 64.43 12 horsebean 160.20 38.63 10 linseed 218.75 52.24 12 meatmeal 276.91 64.90 11 soybean 246.43 54.13 14 sunflower 328.92 48.84 12 Describe the distributions of weights of chickens that were fed linseed and horsebean. Do these data provide strong evidence that the average weights of chickens that were fed linseed and horsebean are different? Use a 5% significance level. What type of error might we have committed? Explain. Would your conclusion change if we used α = 0.01? # (a) # The distribution of chickens fed linseed is normal, whereas that of linseed is slightly skewed. # (b) # The newly hatched chicks were randomly allocated into groups, there are no evidence of dependent relationship between them. # H0: mu_linseed - mu_horsebean = 0 # HA: mu_linseed - mu_horsebean != 0 df = min(12,10) - 1 se = sqrt(52.24^2/12 + 38.63^2/10) t = (218.75-160.2)/se p_value = pt(t, df, lower.tail = FALSE) * 2 # Reject H0. There is strong evidence that average weights of checking fed linseed is different from horsebean. list(df = df, se = se, t = t, p_value = p_value) ## $df ## [1] 9 ## ## $se ## [1] 19.40737 ## ## $t ## [1] 3.016896 ## ## $p_value ## [1] 0.01455232 # (c) # Type I error. Failed to reject H0. # (d) # If alpha is 0.01, we failed to reject H0 with a p-value of 0.015. 5.35 Gaming and distracted eating, Part I. A group of researchers are interested in the possible effects of distracting stimuli during eating, such as an increase or decrease in the amount of food consumption. To test this hypothesis, they monitored food intake for a group of 44 patients who were randomized into two equal groups. The treatment group ate lunch while playing solitaire, and the control group ate lunch without any added distractions. Patients in the treatment group ate 52.1 grams of biscuits, with a standard deviation of 45.1 grams, and patients in the control group ate 27.1 grams of biscuits, with a standard deviation of 26.4 grams. Do these data provide convincing evidence that the average food intake (measured in amount of biscuits consumed) is different for the patients in the treatment group? Assume that conditions for inference are satisfied. # Since the two groups of patients are randomly assigned, they&#39;re independent of each other. # H0: mu_a - mu_b = 0 # HA: mu_a - mu_b != 0 df = 22 - 1 se = sqrt(45.1^2/22 + 26.4^2/22) t = (52.1-27.1-0)/se p_value = pt(t, df, lower.tail = FALSE) * 2 # Yes. list(df = df, se = se, t = t, p_value = p_value) ## $df ## [1] 21 ## ## $se ## [1] 11.14159 ## ## $t ## [1] 2.243845 ## ## $p_value ## [1] 0.03575082 5.37 Prison isolation experiment, Part I. * Subjects from Central Prison in Raleigh, NC, volunteered for an experiment involving an “isolation” experience. The goal of the experiment was to find a treatment that reduces subjects’ psychopathic deviant T scores. This score measures a person’s need for control or their rebellion against control, and it is part of a commonly used mental health test called the Minnesota Multiphasic Personality Inventory (MMPI) test. The experiment had three treatment groups: * (1) Four hours of sensory restriction plus a 15 minute “therapeutic” tape advising that professional help is available. * (2) Four hours of sensory restriction plus a 15 minute “emotionally neutral” tape on training hunting dogs. * (3) Four hours of sensory restriction but no taped message. * Forty-two subjects were randomly assigned to these treatment groups, and an MMPI test was administered before and after the treatment. Distributions of the differences between pre and post treatment scores (pre - post) are shown below, along with some sample statistics. Use this information to independently test the effectiveness of each treatment. Make sure to clearly state your hypotheses, check conditions, and interpret results in the context of the data. / Tr 1 Tr 2 Tr 3 Mean 6.21 2.86 -3.21 SD 12.3 7.94 8.57 n 14 14 14 # The 42 subjects are randomly assigned, hence they&#39;re independent of each other. # Treatment 1 # H0: mu_diff_1 = 0 # HA: mu_diff_1 &gt; 0 df = 13 - 1 se = 12.3/sqrt(14) t = (6.21-0)/se p_value = pt(t, df, lower.tail = FALSE) list(df = df, se = se, t = t, p_value = p_value) ## $df ## [1] 12 ## ## $se ## [1] 3.287313 ## ## $t ## [1] 1.889081 ## ## $p_value ## [1] 0.04164086 # Treatment 2 # H0: mu_diff_2 = 0 # HA: mu_diff_2 &gt; 0 df = 13 - 1 se = 7.94/sqrt(14) t = (2.86-0)/se p_value = pt(t, df, lower.tail = FALSE) list(df = df, se = se, t = t, p_value = p_value) ## $df ## [1] 12 ## ## $se ## [1] 2.122054 ## ## $t ## [1] 1.347751 ## ## $p_value ## [1] 0.1013163 # Treatment 3 # H0: mu_diff_3 = 0 # HA: mu_diff_3 &gt; 0 df = 13 - 1 se = 8.57/sqrt(14) t = (-3.21-0)/se p_value = pt(t, df, lower.tail = FALSE) list(df = df, se = se, t = t, p_value = p_value) ## $df ## [1] 12 ## ## $se ## [1] 2.290429 ## ## $t ## [1] -1.401484 ## ## $p_value ## [1] 0.9068002 5.39 Increasing corn yield. A large farm wants to try out a new type of fertilizer to evaluate whether it will improve the farm’s corn production. The land is broken into plots that produce an average of 1,215 pounds of corn with a standard deviation of 94 pounds per plot. The owner is interested in detecting any average difference of at least 40 pounds per plot. How many plots of land would be needed for the experiment if the desired power level is 90%? Assume each plot of land gets treated with either the current fertilizer or the new fertilizer. # effect size = 40 # power = 0.9 s = 94 # x_old # x_new # se = sqrt( 94^2 / n + 94^2 / n ) # 40 = (qnorm(.9) + abs(qnorm(.05/2))) * se # 40 = 3.24 * sqrt( 94^2 / n + 94^2 / n ) # 40^2 = 3.24^2 * 94^2*2 / n # n = 3.24^2 * 94^2*2 / 40^2 3.24^2 * 94^2*2 / 40^2 ## [1] 115.946 # The sample size should be at least 116 plots # of land per fertilizer. ANOVA and Bootstrapping Comparing More Than Two Means Compare means of 2 groups using a T statistic. Compare means of 3+ groups using a new test called analysis of variance (ANOVA) and a new statistic called F. ANOVA H0: The mean outcome is the same across all categories. HA: At least one pair of means are different from each other. \\[ F = \\frac{\\text{variability between groups}}{\\text{variability within groups}}\\] Obtaining a large F statistic requires that the variability between sample means is greater than the variability within the samples. ANOVA Variability partitioning. Group : Between group variablity. Error : Within group variablity. Degrees of Freedom Total degress of freedom is calculated as sample size minus one. \\[ df_T = n - 1 \\] Group degrees of freedom is calculated as number of groups minus one. \\[ df_G = k - 1 \\] Error degrees of freedom is the difference between the above two DF. \\[ df_E = df_T - df_G \\] Sum of Squares Sum of squares total (SST) measures the total variability in the response variable. Calculated very similarly to variance (except not scaled by the sample size). \\[ SST = \\sum^n_{i=1} (y_i - \\bar{y})^2 \\] \\[ SST = SSG + SSE \\] Sum of squares groups (SSG) measures the variability between groups. It is the explained variability. \\[ SSG = \\sum^k_{j=1} n_j (\\bar{y}_j - \\bar{y})^2 \\] Sum of squares error (SSE) measures the variability within groups. It is the unexplained variability, unexplained by the group variable, due to other reasons. \\[ SSE = SST - SSG \\] Mean Squares Mean sqares is the average variability between and withing groups, calculated as the total variability (sum of squares) scaled by the associated degress of freedom. Mean squares group (MSG) \\[ MSG = \\frac{SSG}{df_G} \\] Mean squares error (MSE) \\[ MSE = \\frac{SSE}{df_E} \\] F-Statistic F-statistic is the ratio of the average between group and within group variabilities. It is never negative. Hence it’s right-skewed. \\[ F = \\frac{MSG}{MSE} \\] P-Value P-value is the probability of at least as large a ratio between the “between” and “within” group variabilities if in fact the means of all groups are equal. Example F-statistics = 21.735 DF_G = 3 DF_E = 791 # If p-value is small (less than alpha), the data provide convincing # evidence that at least one pair of population means are different # from each other (but we can&#39;t tell which one). # If p-value is large, the data do not provide convincing evidence that # at least one pair of population means are different from each other, # the observed differences in sample means are attributable to # sampling variability (or chance). pf(q = 21.735, df1 = 3, df2 = 791, lower.tail = FALSE) ## [1] 1.559855e-13 Conditions for ANOVA Independence Within groups: sampled observations must be independent. Random sample / assignment Each \\(n_j\\) less than 10% of respective population Between groups: the groups must be independent of each other (non-paired). Carefully consider whether the groups may be dependent -&gt; repeated measures anova Approximate normality: distribution should be nearly normal within each group. Especially important when sample sizes are small. Equal variance: groups should have roughly equal variability. Especially important when sample sizes differ between groups. Multiple Comparisons Which means are different? Two sample T tests for differences in each possible pair of groups. Multiple tests will inflate the Type I error rate (\\(\\alpha\\) significance level). Solution: use modified significance level. Testing many pairs of groups is called multiple comparisons. The Bonferroni correction \\(\\alpha^\\star\\) suggests that a more stringent significance level is more appropriate for these tests. Adjust \\(\\alpha\\) by the number of comparisons \\(K\\) being considered. \\[ K = \\frac{k(k-1)}{2} \\] \\[ \\alpha^\\star = \\frac{\\alpha}{K} \\] Constant variance: use consistent standard error and degrees of freedom for all tests. Compare the p-values from each test to the modified significance level. Standard error for multiple pairwise comparisons \\[ SE = \\sqrt{ \\frac{MSE}{n_1} + \\frac{MSE}{n_2} } \\] Degrees of freedom for multiple pairwise comparisons \\[ df = df_E \\] Example If the explanatory variable in an ANOVA has 3 levels, and the F-test in ANOVA yields a significant result, how many pairwise comparisons are needed to compare each group to one another? 3 * (3-1) / 2 ## [1] 3 Example 4 class levels \\(\\alpha\\) = 0.05 for the original ANOVA # Number of comparisons K &lt;- 4 * (4-1) / 2 # Corrected significance level alpha_corrected &lt;- 0.05 / K list(K = K, alpha_corrected = alpha_corrected) ## $K ## [1] 6 ## ## $alpha_corrected ## [1] 0.008333333 Is there a difference between the average vocabulary scores between middle and lower class Americans&gt; (A single pairwise comparison.) DF_E = 691 MSE = 3.628 Lower class N = 41 Mean = 5.07 Middle class N = 331 Mean = 6.76 # H0: mu_middle - mu_lower = 0 # HA: mu_middle - mu_lower != 0 se &lt;- sqrt(3.628/41 + 3.628/331) t &lt;- ((6.76 - 5.07) - 0) / se p_value &lt;- pt(t, df = 791, lower.tail = FALSE) * 2 # P-value is smaller than the alpha 0.00833. Reject the null hypothesis. list(se = se, t = t, p_value = p_value) ## $se ## [1] 0.3153546 ## ## $t ## [1] 5.359046 ## ## $p_value ## [1] 1.09828e-07 Bootstrapping Take a bootstrap sample - a random sample taken with replacement from the original sample, of the same size as the original sample. Calculate the bootstrap statistic - a statistic such as mean, median, proportion, etc. computed on the bootstrap samples. Repeat the above two steps many times to create a bootstrap distribution - a distribution of bootstrap statistics. Percentile method Standard error method Not as rigid conditions as CLT based methods. If the bootstrap distribution is extremely skewed or sparse, the bootstrap interval might be unreliable. A representative sample is still required - if the sample is biased, the estimates resulting from this sample will also be biased. Exercises OpenIntro Statistics, 3rd edition 5.41, 5.43, 5.45, 5.47, 5.49, 5.51 5.41 Fill in the blank. When doing an ANOVA, you observe large differences in means between groups. Within the ANOVA framework, this would most likely be interpreted as evidence strongly favoring the ? hypothesis. # alternative 5.43 Chicken diet and weight, Part III. In Exercises 5.31 and 5.33 we compared the effects of two types of feed at a time. A better analysis would first consider all feed types at once: casein, horsebean, linseed, meat meal, soybean, and sunflower. The ANOVA output below can be used to test for differences between the average weights of chicks on different diets. / Df Sum Sq Mean Sq F value Pr(&gt;F) feed 5 231,129.16 46,225.83 15.36 0.0000 Residuals 65 195,556.02 3,008.55 Conduct a hypothesis test to determine if these data provide convincing evidence that the average weight of chicks varies across some (or all) groups. Make sure to check relevant conditions. Figures and summary statistics are shown below. # H0: The mean outcome is the same across all feed types. # HA: At least one pair of means are different from each other. # F = MSG / MSE f = 46225.83/3008.55 p_value = pf(f, 5, 65, lower.tail = FALSE) list(f = f, p_value = p_value) ## $f ## [1] 15.36482 ## ## $p_value ## [1] 5.936286e-10 5.45 Coffee, depression, and physical activity. Caffeine is the world’s most widely used stimulant, with approximately 80% consumed in the form of coffee. Participants in a study investigating the relationship between coffee consumption and exercise were asked to report the number of hours they spent per week on moderate (e.g., brisk walking) and vigorous (e.g., strenuous sports and jogging) exercise. Based on these data the researchers estimated the total hours of metabolic equivalent tasks (MET) per week, a value always greater than 0. The table below gives summary statistics of MET for women in this study based on the amount of coffee consumed. / ≤ 1 cup/week 2-6 cups/week 1 cup/day 2-3 cups/day ≥ 4 cups/day Total Mean 18.7 19.6 19.3 18.9 17.5 SD 21.1 25.5 22.5 22.0 22.0 n 12,215 6,617 17,234 12,290 2,383 50,739 Write the hypotheses for evaluating if the average physical activity level varies among the different levels of coffee consumption. Check conditions and describe any assumptions you must make to proceed with the test. Below is part of the output associated with this test. Fill in the empty cells. What is the conclusion of the test? # (a) # H0: The mean MET is the same across all groups of coffee consumption. # HA: At least one pair of means is different. # (c) df_total = 50739 - 1 df_coffee = 5 - 1 df_residuals = df_total - df_coffee list( df_total = df_total, df_coffee = df_coffee, df_residuals = df_residuals ) ## $df_total ## [1] 50738 ## ## $df_coffee ## [1] 4 ## ## $df_residuals ## [1] 50734 sum_sq_coffee = 25575327 - 25564819 list(sum_sq_coffee = sum_sq_coffee) ## $sum_sq_coffee ## [1] 10508 mean_sq_coffee = 10508 / df_coffee mean_sq_residuals = 25564819 / df_residuals list(mean_sq_coffee, mean_sq_residuals) ## [[1]] ## [1] 2627 ## ## [[2]] ## [1] 503.8991 # F = MSG / MSE f = mean_sq_coffee / mean_sq_residuals list(f = f) ## $f ## [1] 5.213345 pf(5.21, 4, 50734, lower.tail = FALSE) ## [1] 0.0003412589 / Df Sum Sq Mean Sq F value Pr(&gt;F) coffee 4 10508 2627 5.21 0.0003 Residuals 50734 25,564,819 503.9 / / Total 50738 25,575,327 / / / # (d) # Since p-value is very small, # reject the null hypothesis and conclude that there is # at least one mean of MET that is different. 5.47 GPA and major. Undergraduate students taking an introductory statistics course at Duke University conducted a survey about GPA and major. The side-by-side box plots show the distribution of GPA among three groups of majors. Also provided is the ANOVA output. / Df Sum Sq Mean Sq F value Pr(&gt;F) major 2 0.03 0.015 0.185 0.8313 Residuals 195 15.77 0.081 / / Write the hypotheses for testing for a difference between average GPA across majors. What is the conclusion of the hypothesis test? How many students answered these questions on the survey, i.e. what is the sample size? # (a) # H0: The average GPA across majors is the same. # HA: At least one pair of average GPA is different. # (b) # The p-value is too large and we cannot reject the null hypothesis. # We cannot conclude that there&#39;re any difference between # average GPA across majors. # (c) 195+2+1 ## [1] 198 5.49 True / False: ANOVA, Part I. Determine if the following statements are true or false in ANOVA, and explain your reasoning for statements you identify as false. As the number of groups increases, the modified significance level for pairwise tests increases as well. As the total sample size increases, the degrees of freedom for the residuals increases as well. The constant variance condition can be somewhat relaxed when the sample sizes are relatively consistent across groups. The independence assumption can be relaxed when the total sample size is large. # (a) # K = (k * (k-1)) / 2 # a_modified = a / K # False # The larger the number of groups, the larger the number of comparisons K, # hence the smaller the modified significance level. # (b) # df_residuals = df_total - df_group # df_residuals = (n) - (k - 1) # True # (c) # True # (d) # False 5.51 Prison isolation experiment, Part II. * Exercise 5.37 introduced an experiment that was conducted with the goal of identifying a treatment that reduces subjects’ psychopathic deviant T scores, where this score measures a person’s need for control or his rebellion against control. In Exercise 5.37 you evaluated the success of each treatment individually. An alternative analysis involves comparing the success of treatments. The relevant ANOVA output is given below. / Df Sum Sq Mean Sq F value Pr(&gt;F) treatment 2 639.48 319.74 3.33 0.0461 Residuals 39 3740.43 95.91 s_pooled = 9.793 on df = 39 What are the hypotheses? What is the conclusion of the test? Use a 5% significance level. If in part (b) you determined that the test is significant, conduct pairwise tests to determine which groups are different from each other. If you did not reject the null hypothesis in part (b), recheck your answer. / Tr 1 Tr 2 Tr 3 Mean 6.21 2.86 -3.21 SD 12.3 7.94 8.57 n 14 14 14 # (b) # Since p is smaller than 0.05, reject the null hypothesis # and conclude that at least one pair of mean score is different. # (c) # K = (k * (k-1)) / 2 K = 3 * 2 / 2 # a_modified = a / K a_modified = 0.05 / K list(K = K, a_modified = a_modified) ## $K ## [1] 3 ## ## $a_modified ## [1] 0.01666667 s_pooled = 9.793 df = 39 se = sqrt(9.793^2/14 + 9.793^2/14) # tr 1, tr 2 t1 = (6.21-2.86)/se p1 = pt(t1, df, lower.tail = FALSE) * 2 # tr 1, tr 3 t2 = (6.21-(-3.21))/se p2 = pt(t2, df, lower.tail = FALSE) * 2 # tr 1, tr 3 t3 = (2.86-(-3.21))/se p3 = pt(t3, df, lower.tail = FALSE) * 2 list( se = se, tr1 = list(t = t1, p = p1), tr2 = list(t = t2, p = p2), tr3 = list(t = t3, p = p3) ) ## $se ## [1] 3.701406 ## ## $tr1 ## $tr1$t ## [1] 0.9050615 ## ## $tr1$p ## [1] 0.3709903 ## ## ## $tr2 ## $tr2$t ## [1] 2.544979 ## ## $tr2$p ## [1] 0.01499763 ## ## ## $tr3 ## $tr3$t ## [1] 1.639917 ## ## $tr3$p ## [1] 0.1090649 "],
["week-4.html", "Week 4 Inference for Proportions Simulation Based Inference for Proportions and Chi-Square Testing", " Week 4 Inference for Proportions Inference for proportions works with categorical variables. One categorical variable Two levels: success-failure More than two levels Two categorical variables Two levels: success-failture More than two levels Sampling Variability and CLT for Proportions Central Limit Theorem for a Proportion When observations are independent and the sample size is sufficiently large, the sample proportion \\(\\hat{p}\\) will tend to follow a normal distribution with the following mean and standard error. Mean for a Proportion \\[ \\mu = p \\] Standard Error for a Proportion \\[ SE = \\sqrt{\\frac{p(1-p)}{n}} \\] Conditions for Central Limit Theorem for a Proportion Independence Success-Failure Condition: The sample size should be sufficiently large with \\(np \\ge 10\\) and \\(n(1-p) \\ge 10\\). If the success-failure condition is not met: The center of the sampling distribution will still be around the true population proportion. The spread of the sampling distribution can still be approximated using the same formula for the standard error. The shape of the distribution will depend on whether the true population proportion \\(p\\) is closer to 0 or closer to 1. Example 90% of all plant species are classified as angiosperms. These are flowering plants. If you were to randomly sample 200 plants from the list of all known plant species, what is the probability that at least 95% of the plants in your sample will be flowering plants? p = 0.9 n = 200 # P(p_hat &gt; 0.95)? p_hat = 0.95 # Check the conditions: # 1. Random sampled + 10% of all plants -&gt; Independent # 2. Success-failure condition ans(n_success = n * 0.9, n_failture = n * (1-0.9)) ## 180 ## 20 se = sqrt(p * (1-p) / n) z = (p_hat - p) / se prob = pnorm(z, lower.tail = FALSE) ans(se, z, prob) ## 0.0212132 ## 2.357023 ## 0.009211063 If you were to randomly sample 200 plants from the list of all known plant species, would it be considered unusual if 87.5% of the plants in a random sample of 200 were angiosperms? # 0.875 is within 2 se from the sample mean, # hence it is not unusual. p_hat = 0.875 z = (p_hat - p) / se ans(z) ## -1.178511 What would you expect the shape of the sampling distribution of percentages of angiosperms in random samples of 50 plants to look like? (Remember, 90% of all plants species are classified as angiosperms.) # The success-failture condition does not met # as the number of failure is 5, which is smaller # than 10. The shape of the sampling distribution # will be strongly left skewed as p = 0.9. ans(n_success = 50 * 0.9, n_failture = 50 * 0.1) ## 45 ## 5 Confidence Interval for a Proportion Confidence Interval for a Proportion \\[ \\text{CI} = \\hat{p} \\pm z^\\star SE_{\\hat{p}} \\] ME for a Proportion \\[ \\text{ME} = z^\\star \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] If we have the value of \\(\\hat{p}\\), we can use that in the calculation of the required sample size. If not, use \\(\\hat{p} = 0.5\\). 50-50 is a good guess. It gives the most conservative estimate - highest possible sample size. Example The general social survey found that 571 out of 670, that’s roughly 85% of Americans, answered the question on experiment design correctly. We are asked to estimate using a 95% confidence interval, the proportion of all Americans who have good intuition about experiment design. n = 670 p = 571/670 se = sqrt(p * (1-p) / n) me = 1.96 * se ci = c(p - me, p + me) # We are 95% confident that 82.5% to 88.9% of all Americans # have good intuition about experimental design. ans(p, se, me, ci) ## 0.8522388 ## 0.01370956 ## 0.02687073 ## 0.8253681 ## 0.8791095 The margin of error for this previous confidence interval was 2.7%. If, for a new confidence interval based on a new sample, we wanted to reduce the margin of error to 1% while keeping the confidence level the same. At least how many respondents should we sample? # me = z * se # me = z * sqrt(p * (1 - p) / n) # n = (p * (1 - p)) / (me / z)^2 ans(ceiling(p * (1 - p) / (0.01 / 1.96)^2)) ## 4838 If we wanted to estimate the percentage of Data Analysis and Statistical Inference students who have good intuition about experimental design using a 95% confidence interval and a margin of error no larger than 3%, at least how many students would we need to sample? # me = z * sqrt(p * (1 - p) / n) # n = (p * (1 - p)) / (me / z)^2 n = (0.5 * 0.5) / (0.03 / 1.96)^2 ans(ceiling(n)) ## 1068 Hypothesis Test for a Proportion When we check the success-failure condition for the convidence interval, we use the observed proportion. When we check the success-failure condition for doing a hypothesis test, we use the expected proportion (the null proportion). Hypothesis Testing for a Proportion \\[ H_0: p = \\text{null value}\\\\ H_A: p &lt; or &gt; or\\ne \\text{null value} \\] Example A 2013 Pew Research poll found that 60% of 1,983 randomly sampled American adults believe in evolution. Does this provide convincing evidence that majority of Americans believe in evolution? n = 1983 p = 0.5 p_hat = 0.6 # Randomly sampled + n &lt; 10% of population -&gt; independent # Success-failure condition calculate with expected p (the null p) -&gt; true success_failure = (n * p) &gt; 10 &amp; (n * (1 - p)) &gt; 10 # H0: p = 0.5 # HA: p &gt; 0.5 se = sqrt(p_hat * (1 - p_hat) / n) z = (p_hat - p) / se pvalue = pnorm(z, lower.tail = FALSE) # The p-value is significantly smaller than the significance level 0.05, # hence reject null hypothesis and conclude that there is strong evidence # convincing that the majority (&gt; 0.5) of American adults believe in evolution. # There is almost 0% chance of obtaining a random sample of 1,983 Americans where 60% or more believe in evolution, if in fact 50% of Americans believe # in evolution. ans(success_failure, se, z, pvalue) ## TRUE ## 0.01100131 ## 9.089829 ## 4.959725e-20 Estimating the Difference Between Two Proportions To estimate the difference between two proportions, we label one of our categorical variables the explanatory variable and the other one our response variable. Standard Error for the Difference between Two Proportions \\[ SE = \\sqrt{ \\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2} } \\] Confidence Interval for the Difference between Two Proportions \\[ \\text{CI} = (\\hat{p}_1 - \\hat{p}_2) \\pm z^\\star SE_{(\\hat{p}_1 - \\hat{p}_2)} \\] Conditions for Comparing two Independent Proportions Independence Within groups Random sample / assignment If sampling without replacement, n &lt; 10% of population Between groups The two groups must be independent of each other (non-paired) Success-failure condition Each sample should meet the success-failure condition Example How do Coursera students and the American public at large compare with respect to their views on laws banning possession of handguns? group suc n p_hat US 257 1028 0.25 Coursera 59 84 0.71 p_c = 0.71 n_c = 84 p_us = 0.25 n_us = 1028 se = sqrt(p_c*(1-p_c)/n_c + p_us*(1-p_us)/n_us) p = p_c - p_us ci = c(p - 1.96 * se, p + 1.96 * se) ans(se, p, ci) ## 0.05131845 ## 0.46 ## 0.3594158 ## 0.5605842 Based on the confidence interval we calculated, should we expect to find a significant difference (at the equivalent significance level) between the population proportions of Coursera students and the American public at large who believe there should be a law banning the possession of handguns? # In this hypothesis test the null value for the difference between the two population proportions # would be 0, and 0 isn&#39;t in the interval, hence we should expect to find a difference. Hypothesis Test for Comparing Two Proportions Recall that when we check the success-failure condition for doing a hypothesis test, we use the expected proportion (the null proportion). But for doing a hypothesis test with two proportions, since the null value is \\(H0: p1 = p2\\), we use the pooled proportion. Pooled Proportion \\[ \\begin{align} \\hat{p}_{pool} &amp;= \\frac{\\text{total success}}{\\text{total }n} \\\\ &amp;= \\frac{\\text{# of success}_1 + \\text{# of success}_2} {n_1 + n_2} \\end{align} \\] The success-failure condition is thus \\(n_1 \\hat{p}_{pool} \\ge 10\\), \\(n_1 (1 - \\hat{p}_{pool}) \\ge 10\\), \\(n_2 \\hat{p}_{pool} \\ge 10\\), \\(n_2 (1 - \\hat{p}_{pool}) \\ge 10\\). Standard Error for Hypothesis Test for Comparing Two Proportions \\[ SE = \\sqrt{ \\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_2} } \\] Example Evaluate whether males and females are equally likely to answer “Yes” to the question about whether any of their children have ever been the victim of bullying. group yes no not_sure total p_hat Male 34 52 4 90 0.38 Female 61 61 0 122 0.50 # H0: p_male = p_female # HA: p_male != p_female p_male = 0.38 p_female = 0.50 n_male = 90 n_female = 122 p_pool = (34 + 61) / (n_male + n_female) conditions = n_male * p_pool &gt;= 10 &amp; n_male * (1 - p_pool) &gt;= 10 &amp; n_female * p_pool &gt;= 10 &amp; n_female * (1 - p_pool) &gt;= 10 p = p_male - p_female se = sqrt( p_pool*(1-p_pool)/n_male + p_pool*(1-p_pool)/n_female ) z = (p - 0) / se pvalue = pnorm(z) * 2 ans(p_pool, p, se, z, pvalue) ## 0.4481132 ## -0.12 ## 0.06910121 ## -1.736583 ## 0.08246075 Simulation Based Inference for Proportions and Chi-Square Testing "]
]
